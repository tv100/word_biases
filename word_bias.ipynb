{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Bias\n",
    "\n",
    "Use a pre-trained set of embeddings to detect word biases.\n",
    "\n",
    "You can train your own word embeddings on your dataset. \n",
    "\n",
    "Provided you observe all privacy regulations and you have a self-destructive urge you could use the emails from your organization to check for stereotypes. BAO, training yourself a model is computationally expensive.\n",
    "\n",
    "Recommended lecture: **Sequence Models Specialization by Deeplearning.ai on Coursera.**\n",
    "\n",
    "Or at least the video below:\n",
    "\n",
    "*https://www.coursera.org/lecture/nlp-sequence-models/properties-of-word-embeddings-S2mat*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1145,
     "status": "ok",
     "timestamp": 1594995905166,
     "user": {
      "displayName": "valentin tabus",
      "photoUrl": "",
      "userId": "15218431955538760919"
     },
     "user_tz": -180
    },
    "id": "rZ_yfbKWKV14"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Um5R9_fJKe_H"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    # Cosine similarity (degree of similarity) between the two vectors u and v passed as arguments\n",
    "    # Both vectors hould have the same shape\n",
    "    # If u and v are similar, their cosine similarity will be close to 1\n",
    "    # If they are dissimilar, the result will be less than 1.\n",
    "\n",
    "    assert u.shape[0] == v.shape[0]\n",
    "    cosine =  np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7xeO-7xOTjH"
   },
   "outputs": [],
   "source": [
    "def load_pretrained_word_vector(file = \"data/glove.6B/glove.6B.50d.txt\"):\n",
    "    # load pretrained word vectors from various sources such as:\n",
    "    # https://nlp.stanford.edu/projects/glove/, \n",
    "    # https://fasttext.cc/docs/en/crawl-vectors.html, \n",
    "    # https://www.kaggle.com/, etc.\n",
    "    # used by default a small one with 50 dimensions\n",
    "    \n",
    "    dictionary_embeddings = {}\n",
    "    with open(file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word_embedding_vector = np.asarray(values[1:], \"float64\")\n",
    "            dictionary_embeddings[values[0]] = word_embedding_vector\n",
    "    return dictionary_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy_answer(a, b, c, dictionary_embeddings):\n",
    "    # if \"a is to b as c is to x\" calculate x and return it\n",
    "    # word x is calculated such as the cosine similarity between:\n",
    "    #    - the difference of the word vectors associated to (a, b) and\n",
    "    #    - the difference of the word vectors associated to (c, x)\n",
    "    # is maximal\n",
    "    # word vectors (and their similarity) mirror public data biases\n",
    "\n",
    "    # word vectors associated to the three words a, b and c \n",
    "    va = dictionary_embeddings[a.lower()]\n",
    "    vb = dictionary_embeddings[b.lower()]\n",
    "    vc = dictionary_embeddings[c.lower()]\n",
    "    \n",
    "    words_to_try = dictionary_embeddings.keys() - {a, b, c} # all except a, b and c\n",
    "    max_cosine = -1.0\n",
    "    answer_word = \"_None_\"\n",
    "    \n",
    "    for word in words_to_try:        \n",
    "        cosine = cosine_similarity(vb-va, dictionary_embeddings[word]-vc)\n",
    "        if cosine > max_cosine:\n",
    "            max_cosine = cosine\n",
    "            answer_word = word\n",
    "    return answer_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a pretrained word vectors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in pre-trained model: 400000\n",
      "Each word has associated a 50-dimension vector\n"
     ]
    }
   ],
   "source": [
    "my_dictionary_embeddings = load_pretrained_word_vector(\"data/glove.6B/glove.6B.50d.txt\")\n",
    "print(\"Number of words in pre-trained model: {}\".format(len(my_dictionary_embeddings.keys())))\n",
    "print(\"Each word has associated a {}-dimension vector\".format(my_dictionary_embeddings[\"a\"].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ask some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man is to doctor what woman is to nurse\n"
     ]
    }
   ],
   "source": [
    "question = (\"man\",\"doctor\",\"woman\")\n",
    "print(\"{} is to {} what {} is to {}\".format(*question, word_analogy_answer(*question,my_dictionary_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\"white\",\"doctor\",\"black\")\n",
    "print(\"{} is to {} what {} is to {}\".format(*question, word_analogy_answer(*question,my_dictionary_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\"christian\",\"doctor\",\"muslim\")\n",
    "print(\"{} is to {} what {} is to {}\".format(*question, word_analogy_answer(*question,my_dictionary_embeddings)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMzYQFLrE0O9sW8akkYnzX6",
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
